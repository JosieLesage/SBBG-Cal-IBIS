---
title: "Remove_Duplicate_Data"
author: "Josie Lesage"
date: "12/9/2020"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = TRUE)

# Setup packages
library(rgbif)
library(tidyverse)
library(stringr)
```


## Cleaning data and removing potential duplicates
The code chunks below will remove the duplicated occurrence records for all records gathered from GBIF that are already present in the datasets downloaded from other sources (fungi, arthros, plants). To do this, we will directly call on the other dataset data and remove all duplicates from the data we downloaded from GBIF above. 

Below that, there's a chunk to just extract and save the .csv files to the clean data folder.

****
To begin, use ctrl+F to find and *replace all* of the previous dates (for instance, "Dec 2020") with the new date ("Mar 2020").
****


### Arthropods

```{r removing duplicates from Bugs}
# This line reads the Darwin core occurrence records .txt file that you unzipped from the appropriate folder
## **__THIS IS WHERE YOU WOULD CHANGE THE DATE__**
arthro_raw <- read.delim("Data/Dec 2020/GBIF/Raw/Arthropods/occurrence.txt") 

## --------------- This is the original method - anti-join by codes already pulled out. 
# This line reads the institutional codes CSV, which we will use to remove the dupes from symbiota. Technically we don't need to do this after the first time, but it doesn't hurt to keep consistent. 
# inCode <- read.csv ("PaigeFiles/NA_inCodes_updated2020.csv") %>%   rename(institutionCode = InstitutionCode,         collectionCode = CollectionCode)
# These two lines of code take the downloaded data and remove the records that match the collections and institutions already included in symbiota# These two lines of code take the downloaded data and remove the records that match the collections and institutions already included in symbiota
# arthro_clean1 <- anti_join(arthro_raw, inCode, by = "institutionCode")
# arthro_clean2 <- anti_join(arthro_clean1, inCode, by = "collectionCode")


## --------------- !!!! This is the new way that compares datasets directly, leaves more GBIF in !!!!
scan_clean <- read_csv("Data/Dec 2020/SCAN/SCAN_Clean.csv", 
                       col_types = cols(.default = "c")) %>%
  mutate(identifier = occurrenceID)
ecdysis_clean <- read_csv("Data/Dec 2020/Ecdysis/Ecdysis_Clean.csv", 
                         col_types = cols(.default = "c")) %>%
  mutate(identifier = occurrenceID)

# these steps clean the GBIF plant data by filtering out anything that already has a match in CCH or SEINet.
arthro_process1 <- anti_join(arthro_raw, scan_clean, by = "identifier")
arthro_process2 <- anti_join(arthro_process1, scan_clean, by = "occurrenceID")
arthro_process3 <- anti_join(arthro_process2, scan_clean, by = "institutionCode")
arthro_process4 <- anti_join(arthro_process3, ecdysis_clean, by = "occurrenceID")
arthro_process5 <- anti_join(arthro_process4, ecdysis_clean, by = "identifier")
arthro_process6 <- anti_join(arthro_process5, ecdysis_clean, by = "institutionCode") %>%
  filter(!str_detect(issue, "DIFFERENT_OWNER_INSTITUTION"))

# This line saves the data as a .csv in the appropriate folder with a new name
## **__THIS IS WHERE YOU WOULD CHANGE THE DATE__**
write.csv (arthro_process6, 
           file = "Data/Dec 2020/GBIF/Clean Data/GBIF_arthropods_clean_Feb21.csv",
           na="", row.names=FALSE)
```

### Fungi

```{r removing duplicates from Fungi}
# This line reads the Darwin core occurrence records .txt file that you unzipped from the appropriate folder
## **__THIS IS WHERE YOU WOULD CHANGE THE DATE__**
fungi_raw <- read.delim("Data/Dec 2020/GBIF/Raw/Fungi/occurrence.txt")

## --------------- This is the original method - anti-join by codes already pulled out. 
# This line reads the institutional codes CSV, which we will use to remove the dupes from symbiota. Technically we don't need to do this after the first time, but it doesn't hurt to keep consistent. 
# inCode <- read.csv ("PaigeFiles/NA_inCodes_updated2020.csv") %>%   rename(institutionCode = InstitutionCode,         collectionCode = CollectionCode)
# These two lines of code take the downloaded data and remove the records that match the collections and institutions already included in symbiota# These two lines of code take the downloaded data and remove the records that match the collections and institutions already included in symbiota
# fungi_clean1 <- anti_join(fungi_raw, inCode, by = "institutionCode")
# fungi_clean2 <- anti_join(fungi_clean1, inCode, by = "collectionCode")

## --------------- !!!! This is the new way that compares datasets directly, leaves more GBIF in !!!!
MyCo_clean <- read_csv("Data/Dec 2020/MyCoPortal/MyCoPortal_Clean.csv", 
                       col_types = cols(.default = "c")) %>%
  mutate(identifier = occurrenceID)

# these steps clean the GBIF fungi data by filtering out anything that already has a match  in MyCoPortal
fungi_process1 <- anti_join(fungi_raw, MyCo_clean, by = "identifier")
fungi_process2 <- anti_join(fungi_process1, MyCo_clean, by = "occurrenceID")
fungi_process3 <- anti_join(fungi_process2, MyCo_clean, by = "institutionCode") %>%
  filter(!str_detect(issue, "DIFFERENT_OWNER_INSTITUTION"))

# This line saves the data as a .csv in the appropriate folder with a new name
## **__THIS IS WHERE YOU WOULD CHANGE THE DATE__**
write.csv (fungi_process3, 
           file = "Data/Dec 2020/GBIF/Clean Data/GBIF_fungi_clean_Dec 2020.csv",
           na="", row.names=FALSE)

```

### Plants

```{r removing duplicates from Plants}
# This line reads the Darwin core occurrence records .txt file that you unzipped from the appropriate folder
## **__THIS IS WHERE YOU WOULD CHANGE THE DATE__**
plant_raw <- read_tsv("Data/Dec 2020/GBIF/Raw/Plants/occurrence.txt", col_types = cols(.default = "c"))



## --------------- This is the original method - anti-join by codes already pulled out. 
# This line reads the institutional codes CSV, which we will use to remove the dupes from symbiota. Technically we don't need to do this after the first time, but it doesn't hurt to keep consistent. 
# inCode <- read.csv ("PaigeFiles/NA_inCodes_updated2020.csv") %>%   rename(institutionCode = InstitutionCode,         collectionCode = CollectionCode)
# These two lines of code take the downloaded data and remove the records that match the collections and institutions already included in symbiota
# plant_clean1 <- anti_join(plant_raw, inCode, by = "institutionCode")
# plant_clean2 <- anti_join(plant_clean1, inCode, by = "collectionCode")


## --------------- !!!! This is the new way that compares datasets directly, leaves more GBIF in !!!!
cch2_clean <- read_csv("Data/Dec 2020/CCH2/CCH_Clean.csv", col_types = cols(.default = "c")) %>%
  mutate(identifier = occurrenceID)
seinet_clean <- read_csv("Data/Dec 2020/SEINet/SEINet_Clean.csv", col_types = cols(.default = "c")) %>%
  mutate(identifier = occurrenceID)

# these steps clean the GBIF plant data by filtering out anything that already has a match in CCH or SEINet.
plant_process1 <- anti_join(plant_raw, cch2_clean, by = "identifier")
plant_process2 <- anti_join(plant_process1, cch2_clean, by = "occurrenceID")
plant_process3 <- anti_join(plant_process2, cch2_clean, by = "institutionCode")
plant_process4 <- anti_join(plant_process3, seinet_clean, by = "occurrenceID")
plant_process5 <- anti_join(plant_process4, seinet_clean, by = "identifier")
plant_process6 <- anti_join(plant_process5, seinet_clean, by = "institutionCode") %>%
  filter(!str_detect(issue, "DIFFERENT_OWNER_INSTITUTION"))

# This line saves the data as a .csv in the appropriate folder with a new name
## **__THIS IS WHERE YOU WOULD CHANGE THE DATE__**
write.csv (plant_process6, file = "Data/Dec 2020/GBIF/Clean Data/GBIF_plants_clean_Dec 2020.csv", na="", row.names=FALSE)
```

## Cut up the big files into multiple parts
Cutting the large datasets into smaller pieces makes it easier to upload to symbiota. 
**In the future, we may need to increase the number of slices.**

```{r cut up big files}
amphibia <- read.delim("Data/Dec 2020/GBIF/Raw/Amphibia/occurrence.txt", quote = "")
mammalia <- read.delim("Data/Dec 2020/GBIF/Raw/Mammals/occurrence.txt", quote = "")
reptilia <- read.delim("Data/Dec 2020/GBIF/Raw/Reptiles/occurrence.txt", quote = "")
aves <- read.delim("Data/Dec 2020/GBIF/Raw/Aves/occurrence.txt", quote = "")
fish <- read.delim("Data/Dec 2020/GBIF/Raw/Fish/occurrence.txt", quote = "")
inverts <- read.delim("Data/Dec 2020/GBIF/Raw/Inverts/occurrence.txt", quote = "")

# There are just over 300,000 bird observations, which we'll split into 4 files for easy upload.
aves1 <- slice(aves, 1:100000)
aves2 <- slice(aves, 100001:200000)
aves3 <- slice(aves, 200001:300000)
aves4 <- slice(aves, 300001:400000)

# Split fish in half
fish1 <- slice(fish, 1:100000)
fish2 <- slice(fish, 100001:200000)
```



## Save the remaining .csv files
For the other groups, we just want to save everything to the correct folder. Don't forget to change the date in the second half of the code for each group.

```{r}
## **__THIS IS WHERE YOU WOULD CHANGE THE DATES FOR EACH GROUP__**
write.csv (amphibia, file = "Data/Dec 2020/GBIF/Clean Data/GBIF_amphibia_clean_Dec 2020.csv", na="", row.names=FALSE)
write.csv (mammalia, file = "Data/Dec 2020/GBIF/Clean Data/GBIF_mammalia_clean_Dec 2020.csv", na="", row.names=FALSE)
write.csv (reptilia, file = "Data/Dec 2020/GBIF/Clean Data/GBIF_reptilia_clean_Dec 2020.csv", na="", row.names=FALSE)
write.csv (aves1, file = "Data/Dec 2020/GBIF/Clean Data/GBIF_aves1_clean_Dec 2020.csv", na="", row.names=FALSE)
write.csv (aves2, file = "Data/Dec 2020/GBIF/Clean Data/GBIF_aves2_clean_Dec 2020.csv", na="", row.names=FALSE)
write.csv (aves3, file = "Data/Dec 2020/GBIF/Clean Data/GBIF_aves3_clean_Dec 2020.csv", na="", row.names=FALSE)
write.csv (aves4, file = "Data/Dec 2020/GBIF/Clean Data/GBIF_aves4_clean_Dec 2020.csv", na="", row.names=FALSE)
write.csv (fish1, file = "Data/Dec 2020/GBIF/Clean Data/GBIF_fish1_clean_Dec 2020.csv", na="", row.names=FALSE)
write.csv (fish2, file = "Data/Dec 2020/GBIF/Clean Data/GBIF_fish2_clean_Dec 2020.csv", na="", row.names=FALSE)
write.csv (inverts, file = "Data/Dec 2020/GBIF/Clean Data/GBIF_inverts_clean_Dec 2020.csv", na="", row.names=FALSE)

```

Voila! You did it!

You'll need to compress any files over 100 kb (just make them a zip folder) before uploading to Cal-IBIS.