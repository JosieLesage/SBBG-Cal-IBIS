---
title: "Remove_Duplicate_Data"
author: "Josie Lesage"
date: "12/9/2020"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = TRUE)

# Setup packages
library(rgbif)
library(tidyverse)
library(stringr)
```


## Cleaning data and removing potential duplicates
The code chunks below will remove the duplicated occurrence records for all records gathered from GBIF that are already present in the datasets downloaded from other sources (fungi, arthros, plants). To do this, we will directly call on the other dataset data and remove all duplicates from the data we downloaded from GBIF above. 

Below that, there's a chunk to just extract and save the .csv files to the clean data folder.

****
To begin, use ctrl+F to find and *replace all* of the previous dates (for instance, "Dec 2020") with the new date ("Mar 2020").
****


### Arthropods

```{r removing duplicates from Bugs}
# This line reads the Darwin core occurrence records .txt file that you unzipped from the appropriate folder
## **__THIS IS WHERE YOU WOULD CHANGE THE DATE__**
arthro_raw <- read.delim("Data/Dec 2020/GBIF/Raw/Arthropods/occurrence.txt")

# This line reads the institutional codes CSV, which we will use to remove the dupes from symbiota. Technically we don't need to do this after the first time, but it doesn't hurt to keep consistent.
inCode <- read.csv ("NA_inCodes_updated2020.csv") %>%
  rename(institutionCode = InstitutionCode,
         collectionCode = CollectionCode)

# These two lines of code take the downloaded data and remove the records that match the collections and institutions already included in symbiota
arthro_clean1 <- anti_join(arthro_raw, inCode, by = "institutionCode")
arthro_clean2 <- anti_join(arthro_clean1, inCode, by = "collectionCode")

# This line saves the data as a .csv in the appropriate folder with a new name
## **__THIS IS WHERE YOU WOULD CHANGE THE DATE__**
write.csv (arthro_clean2, file = "Data/Dec 2020/GBIF/Clean Data/GBIF_arthropods_clean_Dec 2020.csv", na="", row.names=FALSE)
```

### Fungi

```{r removing duplicates from Fungi}
# This line reads the Darwin core occurrence records .txt file that you unzipped from the appropriate folder
## **__THIS IS WHERE YOU WOULD CHANGE THE DATE__**
fungi_raw <- read.delim("Data/Dec 2020/GBIF/Raw/Fungi/occurrence.txt")

# This line reads the institutional codes CSV, which we will use to remove the dupes from symbiota. Technically we don't need to do this after the first time, but it doesn't hurt to keep consistent.
inCode <- read.csv ("NA_inCodes_updated2020.csv") %>%
  rename(institutionCode = InstitutionCode,
         collectionCode = CollectionCode)

# These two lines of code take the downloaded data and remove the records that match the collections and institutions already included in symbiota
fungi_clean1 <- anti_join(fungi_raw, inCode, by = "institutionCode")
fungi_clean2 <- anti_join(fungi_clean1, inCode, by = "collectionCode")

# This line saves the data as a .csv in the appropriate folder with a new name
## **__THIS IS WHERE YOU WOULD CHANGE THE DATE__**
write.csv (fungi_clean2, file = "Data/Dec 2020/GBIF/Clean Data/GBIF_fungi_clean_Dec 2020.csv", na="", row.names=FALSE)
```

### Plants

```{r removing duplicates from Plants}
# This line reads the Darwin core occurrence records .txt file that you unzipped from the appropriate folder
## **__THIS IS WHERE YOU WOULD CHANGE THE DATE__**
plant_raw <- read_tsv("Data/Dec 2020/GBIF/Raw/Plants/occurrence.txt", col_types = cols(.default = "c"))

# This line reads the institutional codes CSV, which we will use to remove the dupes from symbiota. Technically we don't need to do this after the first time, but it doesn't hurt to keep consistent. 
inCode <- read.csv ("NA_inCodes_updated2020.csv") %>%
  rename(institutionCode = InstitutionCode,
         collectionCode = CollectionCode)

# These two lines of code take the downloaded data and remove the records that match the collections and institutions already included in symbiota

## --------------- !!!! THIS IS THE ORIGINAL WAY !!!!
plant_clean1 <- anti_join(plant_raw, inCode, by = "institutionCode")
plant_clean2 <- anti_join(plant_clean1, inCode, by = "collectionCode")

## --------------- !!!! THIS WAY COMPARES THE CCH2 AND SEINET DATA DIRECTLY !!!!

cch2_clean <- read_csv("Data/Dec 2020/CCH2/CCH_Clean.csv", col_types = cols(.default = "c")) %>%
  mutate(identifier = occurrenceID)
seinet_clean <- read_csv("Data/Dec 2020/SEINet/SEINet_Clean.csv", col_types = cols(.default = "c")) %>%
  mutate(identifier = occurrenceID)

plant_process1 <- anti_join(plant_raw, cch2_clean, by = "identifier")
plant_process2 <- anti_join(plant_process1, cch2_clean, by = "occurrenceID")
plant_process3 <- anti_join(plant_process2, cch2_clean, by = "institutionCode")
plant_process4 <- anti_join(plant_process3, seinet_clean, by = "occurrenceID")
plant_process5 <- anti_join(plant_process4, seinet_clean, by = "identifier")
plant_process6 <- anti_join(plant_process5, seinet_clean, by = "institutionCode") %>%
  filter(!str_detect(issue, "DIFFERENT_OWNER_INSTITUTION"))

# This line saves the data as a .csv in the appropriate folder with a new name
## **__THIS IS WHERE YOU WOULD CHANGE THE DATE__**
write.csv (plant_process6, file = "Data/Dec 2020/GBIF/Clean Data/GBIF_plants_clean_Dec 2020.csv", na="", row.names=FALSE)
```


If you run into the "Error in file(file, "rt") : cannot open the connection" error, check that you unzipped the files!


## Cut up the big files into multiple parts

```{r cut up big files}
amphibia <- read.delim("Data/Dec 2020/GBIF/Raw/Amphibia/occurrence.txt", quote = "")
mammalia <- read.delim("Data/Dec 2020/GBIF/Raw/Mammals/occurrence.txt", quote = "")
reptilia <- read.delim("Data/Dec 2020/GBIF/Raw/Reptiles/occurrence.txt", quote = "")
aves <- read.delim("Data/Dec 2020/GBIF/Raw/Aves/occurrence.txt", quote = "")
fish <- read.delim("Data/Dec 2020/GBIF/Raw/Fish/occurrence.txt", quote = "")
inverts <- read.delim("Data/Dec 2020/GBIF/Raw/Inverts/occurrence.txt", quote = "")

# There are just over 300,000 bird observations, which we'll split into 4 files for easy upload.
aves1 <- slice(aves, 1:100000)
aves2 <- slice(aves, 100001:200000)
aves3 <- slice(aves, 200001:300000)
aves4 <- slice(aves, 300001:400000)

# Split fish in half
fish1 <- slice(fish, 1:100000)
fish2 <- slice(fish, 100001:200000)
```



## Save the remaining .csv files
For the other groups, we just want to save everything to the correct folder. Don't forget to change the date in the second half of the code for each group.

```{r}


## **__THIS IS WHERE YOU WOULD CHANGE THE DATES FOR EACH GROUP__**
write.csv (amphibia, file = "Data/Dec 2020/GBIF/Clean Data/GBIF_amphibia_clean_Dec 2020.csv", na="", row.names=FALSE)
write.csv (mammalia, file = "Data/Dec 2020/GBIF/Clean Data/GBIF_mammalia_clean_Dec 2020.csv", na="", row.names=FALSE)
write.csv (reptilia, file = "Data/Dec 2020/GBIF/Clean Data/GBIF_reptilia_clean_Dec 2020.csv", na="", row.names=FALSE)
write.csv (aves1, file = "Data/Dec 2020/GBIF/Clean Data/GBIF_aves1_clean_Dec 2020.csv", na="", row.names=FALSE)
write.csv (aves2, file = "Data/Dec 2020/GBIF/Clean Data/GBIF_aves2_clean_Dec 2020.csv", na="", row.names=FALSE)
write.csv (aves3, file = "Data/Dec 2020/GBIF/Clean Data/GBIF_aves3_clean_Dec 2020.csv", na="", row.names=FALSE)
write.csv (aves4, file = "Data/Dec 2020/GBIF/Clean Data/GBIF_aves4_clean_Dec 2020.csv", na="", row.names=FALSE)
write.csv (fish1, file = "Data/Dec 2020/GBIF/Clean Data/GBIF_fish1_clean_Dec 2020.csv", na="", row.names=FALSE)
write.csv (fish2, file = "Data/Dec 2020/GBIF/Clean Data/GBIF_fish2_clean_Dec 2020.csv", na="", row.names=FALSE)
write.csv (inverts, file = "Data/Dec 2020/GBIF/Clean Data/GBIF_inverts_clean_Dec 2020.csv", na="", row.names=FALSE)

```


Voila! You did it!

You'll need to compress any files over 100 kb (just make them a zip folder) before uploading to Cal-IBIS.